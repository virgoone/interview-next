---
title: AI 应用进阶
---

# AI 应用进阶（RAG + Agent + Fine-tuning）

## 主问题：RAG、Agent、Fine-tuning 的关联与实操方法？

### 核心回答

三者是"增强能力→赋予执行→场景适配"的递进关系：

- **RAG（检索增强生成）**：为 Agent 提供"可靠外部知识源"，解决 LLM 知识过期、事实幻觉问题，让 Agent 决策有依据；

- **Agent（智能体）**：为 RAG 提供"自主调用能力"，Agent 可判断是否需要检索、检索关键词、检索失败后如何调整（如扩大范围、更换关键词），避免 RAG 被动调用；

- **Fine-tuning（微调）**：让 RAG 与 Agent 协同更高效，核心优化三大方向——①Agent 的决策逻辑（如何时调用 RAG、如何拆解任务）；②RAG 的检索精度（如领域适配的 Embeddings 模型）；③两者的协同效率（如 Agent 如何高效利用检索结果），最终实现"精准检索+智能执行"的场景化落地。

### 延伸问题-回答

1. **延伸问题**：先微调 LLM 还是先优化 RAG？

回答：优先优化 RAG，再考虑微调 LLM。原因：RAG 的检索精度是 Agent 决策的基础——若检索结果本身不准确，再微调 LLM 也只能"基于错误信息做决策"，属于无效优化。建议先通过换领域 Embeddings、加重量排、优化检索参数提升 RAG 效果，若仍存在"Agent 不会用检索结果""决策逻辑混乱"，再进行 LLM 轻量微调（如 LoRA），性价比更高。

2. **延伸问题**：微调后 RAG 和 Agent 脱节（比如 Agent 不调用 RAG）怎么解决？

回答：核心是"强化 RAG-Agent 协同逻辑"——①补充协同样本：新增"必须调用 RAG"的场景样本（如涉及最新政策、产品价格、专业术语的问题）；②明确触发条件：在 Prompt 中固化检索规则（如"用户问题包含'2024 年''最新''政策'等关键词，必须先调用知识库"）；③添加失败处理样本：补充"检索无结果时，Agent 应调整关键词重新检索，而非直接拒绝"的样本，让模型形成条件反射。

3. **延伸问题**：领域数据不足（仅几百条）怎么做好 Fine-tuning？

回答：采用"少量标注+大量无标注"的混合策略——①无标注数据利用：用领域内无标注文档做 Embeddings 模型的自监督预训练（如对比学习），提升语义捕捉能力；②模拟样本生成：用 GPT-4 基于 100 条真实数据，生成 1000 条模拟的任务拆解/工具调用样本；③轻量微调：用 LoRA 微调 LLM，冻结主干参数仅训适配器，减小学习率（如 2e-5）、增加训练轮数，避免过拟合；④迁移学习：先在通用领域数据上训基础适配器，再用领域数据微调，快速适配场景。

4. **延伸问题**：RAG-Agent 的 Fine-tuning 和单纯 LLM 的 Fine-tuning 有何区别？

回答：核心差异在"优化目标"与"数据要求"——①优化目标：单纯 LLM 微调侧重"补充知识"和"通用生成能力"；RAG-Agent 微调侧重"优化决策逻辑"和"协同能力"，让模型学会"何时用工具、如何用检索结果、如何拆解任务"；②数据要求：单纯 LLM 微调只需"输入-输出对"；RAG-Agent 微调需要"多组件联动数据"（如检索结果、工具调用日志、任务拆解步骤），数据需贴近真实场景流程。

> （注：文档部分内容可能由 AI 生成）

